# Analysis of Code and Test Generation Using LLMs

A 2024-25 group term project for the BLG 475E Software Quality and Testing course given in ITU.

## Project Description

The first phase of this project focuses on analyzing the correctness and reliability of codes and unit tests generated by LLMs (Large Language Models) without modifying the codes and only focusing on the output given by the AI model. The LLMs we chose for this project were ChatGPT o3 and Gemini 2.5 Pro, for their recency and superior capabilities among all LLMs. 30 prompts were chosen from the HumanEval dataset and given to both LLMs in order to generate codes and tests. The results obtained from the tests were recorded for further analysis. This repository also includes test coverage reports in various formats.

## Installation
To run this project locally, clone the repository:
```
git clone https://github.com/g-akca/llm-testing.git
cd llm-testing
```

## Tech Stack
- **Python 3.x**
- **Testing Framework:** unittest
- **LLMs Utilized:** ChatGPT o3, Gemini 2.5 Pro

## Acknowledgements
- [HumanEval: Hand-Written Evaluation Set](https://github.com/openai/human-eval)
