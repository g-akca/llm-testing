# Analysis of Code and Test Generation Using LLMs

A 2024-25 group term project for the BLG 475E Software Quality and Testing course given in ITU.

## Project Description

The first phase of this project focuses on analyzing the correctness and reliability of codes and unit tests generated by LLMs (Large Language Models) without modifying the codes and only focusing on the output given by the AI model. The LLMs we chose for this project were ChatGPT o3 and Gemini 2.5 Pro, for their recency and superior capabilities among all LLMs. 30 prompts were chosen from the HumanEval dataset and given to both LLMs in order to generate codes and tests. The results obtained from the tests were recorded for further analysis. This repository also includes test coverage reports in various formats.

The second phase focuses on refining and evaluating LLM-generated code by improving prompt quality and expanding test coverage. Firstly the initial code and unit tests were analyzed, identifying missing edge and boundary cases. New unit tests were created for each prompt and executed against both GPT and Gemini-generated code. Instead of editing the generated code directly, we enhanced the clarity of the original HumanEval prompts to guarantee better code generation. The modified prompts were then used to regenerate code and rerun tests to assess improvements. Finally, for integration testing, we selected three prompts and combined them into a class-based implementation. Integration tests were generated both by LLMs and manually.

## Installation
To run this project locally, clone the repository:
```
git clone https://github.com/g-akca/llm-testing.git
cd llm-testing
```

## Tech Stack
- **Python 3.x**
- **Testing Framework:** unittest
- **LLMs Utilized:** ChatGPT o3, Gemini 2.5 Pro

## ğŸ“ Repository Structure

- `phase-1/` â€“ Initial code and tests generated by LLMs
  - `easy/` â€“ Easy difficulty level prompts' LLM-generated code and tests
  - `hard/` â€“ Hard difficulty level prompts' LLM-generated code and tests
  - `medium/` â€“ Medium difficulty level prompts' LLM-generated code and tests

- `phase-2/` â€“ Human-written tests, regenerated code/tests, integration testing
  - `easy/` â€“ Same as phase-1 folder + regenerated codes and human-written tests
  - `hard/` â€“ Same as phase-1 folder + regenerated codes and human-written tests
  - `medium/` â€“ Same as phase-1 folder + regenerated codes and human-written tests
  - `integration/` â€“ Combined prompt implementations and tests

- `coverage.csv` â€“ Test coverage report in CSV format
- `coverage.json` â€“ Test coverage report in JSON format
- `coverage.py` â€“ Script for analyzing test coverage
- `humaneval.json` â€“ HumanEval dataset
- `run_human_tests.py` â€“ Script to run the human-written unit tests

## Acknowledgements
- [HumanEval: Hand-Written Evaluation Set](https://github.com/openai/human-eval)
